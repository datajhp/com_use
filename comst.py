import os
import json
import time
import requests
import streamlit as st
from PyPDF2 import PdfReader, PdfWriter
import fitz  # PyMuPDF

# ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_DIR = "./"
UPLOAD_DIR = os.path.join(BASE_DIR, "uploads")
SPLIT_DIR = os.path.join(BASE_DIR, "splits")
JSON_DIR = os.path.join(BASE_DIR, "jsons")
RESULT_DIR = os.path.join(BASE_DIR, "results")

for d in [UPLOAD_DIR, SPLIT_DIR, JSON_DIR, RESULT_DIR]:
    os.makedirs(d, exist_ok=True)

# secrets.tomlì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
API_KEY = st.secrets["api"]["upstage_key"]
OCR_URL = "https://api.upstage.ai/v1/document-digitization"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

# ê³ ê¸‰ ë¶„í•  ê°œìˆ˜ ì¶”ì²œ í•¨ìˆ˜
def recommend_split_count_advanced(pdf_path):
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)
    reader = PdfReader(pdf_path)
    total_pages = len(reader.pages)
    avg_size_per_page = file_size_mb / total_pages if total_pages else 0

    doc = fitz.open(pdf_path)
    image_page_count = sum(1 for page in doc if page.get_images())
    image_ratio = image_page_count / total_pages if total_pages else 0
    doc.close()

    recommended = 8
    if total_pages <= 10:
        recommended = 1
    elif total_pages <= 30:
        recommended = 3
    elif total_pages <= 60:
        recommended = 6
    elif total_pages <= 100:
        recommended = 8
    elif total_pages <= 150:
        recommended = 10
    else:
        recommended = min(15, total_pages // 10)

    if avg_size_per_page > 1.5:
        recommended += 2
    elif avg_size_per_page > 1.0:
        recommended += 1

    if image_ratio > 0.7:
        recommended += 2
    elif image_ratio > 0.4:
        recommended += 1

    return min(recommended, total_pages)

# PDF ë¶„í•  ë²”ìœ„ ê³„ì‚°
def generate_split_ranges(total_pages, num_parts):
    base = total_pages // num_parts
    ranges = []
    for i in range(num_parts):
        start = i * base + 1
        end = (i + 1) * base if i < num_parts - 1 else total_pages
        ranges.append((start, end))
    return ranges

# PDF ë¶„í• 
def split_pdf(input_path, output_dir, num_parts):
    reader = PdfReader(input_path)
    total_pages = len(reader.pages)
    split_ranges = generate_split_ranges(total_pages, num_parts)
    split_paths = []

    for idx, (start, end) in enumerate(split_ranges):
        writer = PdfWriter()
        for page_num in range(start - 1, end):
            writer.add_page(reader.pages[page_num])
        output_pdf_path = os.path.join(output_dir, f"split_{idx+1}.pdf")
        with open(output_pdf_path, "wb") as f:
            writer.write(f)
        split_paths.append(output_pdf_path)
    return split_paths

# OCR API í˜¸ì¶œ ë° ì €ì¥ (ì¬ì‹œë„ í¬í•¨)
def call_api_until_success(pdf_path, output_json_path, max_retries=5):
    for attempt in range(max_retries):
        try:
            with open(pdf_path, "rb") as f:
                files = {"document": f}
                data = {"ocr": "force", "base64_encoding": "['table']", "model": "document-parse"}
                response = requests.post(OCR_URL, headers=HEADERS, files=files, data=data)

            if response.status_code == 200:
                result = response.json()
                with open(output_json_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                return True
        except Exception as e:
            st.warning(f"ì˜ˆì™¸ ë°œìƒ: {e}")
        time.sleep(2)
    return False

# JSON ë³‘í•©
def merge_jsons(input_dir, output_path):
    merged_html = ""
    for filename in sorted(os.listdir(input_dir)):
        if filename.endswith(".json"):
            with open(os.path.join(input_dir, filename), "r", encoding="utf-8") as f:
                data = json.load(f)
                try:
                    html = data["content"]["html"]
                    merged_html += html + "\n"
                except KeyError:
                    st.warning(f"HTML ëˆ„ë½: {filename}")
    result = {"api": "2.0", "content": {"html": merged_html}}
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    return output_path

# Streamlit UI
st.set_page_config(page_title="ì»´í™œ ìš”ì•½ì§‘ OCR ìë™í™”", layout="wide")
st.title("ğŸ“„ ì»´í™œ ìš”ì•½ì§‘ ìë™ ìƒì„±ê¸°")

uploaded_file = st.file_uploader("1. PDF íŒŒì¼ ì—…ë¡œë“œ", type="pdf")

if uploaded_file:
    pdf_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.read())
    st.success("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ")

    # ì¶”ì²œ ë¶„í•  ê°œìˆ˜
    recommended = recommend_split_count_advanced(pdf_path)
    st.info(f"ğŸ” ì¶”ì²œ ì•ˆì „ ë¶„í•  ê°œìˆ˜: {recommended}ê°œ (í˜ì´ì§€ ìˆ˜, ì´ë¯¸ì§€ ë¹„ìœ¨, í•´ìƒë„ ê¸°ì¤€)")

    num_parts = st.slider("2. ë¶„í•  ê°œìˆ˜ ì„ íƒ", min_value=1, max_value=20, value=recommended)

    if st.button("3. OCR ì‹œì‘"):
        st.info("ğŸ”§ PDF ë¶„í•  ì¤‘...")
        split_paths = split_pdf(pdf_path, SPLIT_DIR, num_parts)
        st.success(f"ğŸ“„ ì´ {len(split_paths)}ê°œë¡œ ë¶„í•  ì™„ë£Œ")

        for i, path in enumerate(split_paths):
            json_path = os.path.join(JSON_DIR, f"split_{i+1}.json")
            with st.spinner(f"ğŸ” OCR ì¤‘: split_{i+1}.pdf"):
                success = call_api_until_success(path, json_path)
                if success:
                    st.success(f"âœ… ì™„ë£Œ: split_{i+1}")
                else:
                    st.error(f"âŒ ì‹¤íŒ¨: split_{i+1}")

        st.info("ğŸ§© OCR ê²°ê³¼ ë³‘í•© ì¤‘...")
        merged_path = os.path.join(RESULT_DIR, "merged_output.json")
        merged = merge_jsons(JSON_DIR, merged_path)
        st.success("âœ… ë³‘í•© ì™„ë£Œ")

        with open(merged, "rb") as f:
            st.download_button("ğŸ“¥ ë³‘í•©ëœ JSON ë‹¤ìš´ë¡œë“œ", f, file_name="merged_output.json", mime="application/json")





import streamlit as st
import json
import openai

# ê¸°ì¡´ API Key ì‚¬ìš©
from main import API_KEY  # ë˜ëŠ” ë³„ë„ config.pyë¡œ ë¶„ë¦¬ ê°€ëŠ¥

openai.api_key = API_KEY

# ì œëª©
st.title("ğŸ§  ì»´í™œ ìš”ì•½ ìë™ ìƒì„±ê¸° (GPT)")

# JSON ì—…ë¡œë“œ
json_file = st.file_uploader("ğŸ“¤ OCR ê²°ê³¼ merged_output.json ì—…ë¡œë“œ", type="json")
if json_file:
    data = json.load(json_file)
    html_text = data["content"]["html"]

    # ê³¼ëª©/ì¥ ì •ë³´ ì…ë ¥
    subject = st.text_input("ê³¼ëª©", "1ê³¼ëª©")
    chapter = st.text_input("ì¥ ì •ë³´", "2ì¥, 3ì¥, 4ì¥, 5ì¥")

    sections = [
        "ì„¤ì •</h1>", "ìœ Â·ë¬´ì„  ë„¤íŠ¸ì›Œí¬ ì„¤ì •</h1>", "ì»´í“¨í„°ì˜ ê°œë… ë° ì›ë¦¬</h1>", "ì»´í“¨í„°ì˜ ë°œì „ ê³¼ì •</h1>",
        "ì»´í“¨í„°ì˜ ë¶„ë¥˜</h1>", "ìë£Œì˜ í‘œí˜„ ë° ì²˜ë¦¬ ë°©ì‹</h1>", "ìˆ˜ì˜ í‘œí˜„ ë° ì—°ì‚°</h1>", "ì¤‘ì•™ ì²˜ë¦¬ ì¥ì¹˜</h1>",
        "ê¸°ì–µ ì¥ì¹˜ì˜ êµ¬ì„±</h1>", "ì…ì¶œë ¥ ì¥ì¹˜</h1>", "ê¸°íƒ€ ì¥ì¹˜</h1>", "ì†Œí”„íŠ¸ì›¨ì–´</h1>", "ìœ í‹¸ë¦¬í‹°(Utility)</h1>",
        "í”„ë¡œê·¸ë˜ë° ì–¸ì–´</h1>", "PC ìœ ì§€ì™€ ë³´ìˆ˜</h1>", "Windowsì—ì„œ PC ê´€ë¦¬\n", "ì¸í„°ë„· ì¼ë°˜</h1>",
        "ì¸í„°ë„· ì„œë¹„ìŠ¤</h1>", "ë©€í‹°ë¯¸ë””ì–´ì˜ ê°œë…\n", "ë©€í‹°ë¯¸ë””ì–´ì˜ ìš´ìš©</h1>", "ì •ë³´ í†µì‹  ì¼ë°˜</h1>",
        "ì •ë³´ ìœ¤ë¦¬</h1>", "ì»´í“¨í„° ë²”ì£„</h1>", "ë°”ì´ëŸ¬ìŠ¤ ì˜ˆë°©ê³¼ ì¹˜ë£Œ</h1>",
    ]

    # ì ˆ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
    def extract_section(section_title):
        start_idx = html_text.find(section_title)
        if start_idx == -1:
            return None
        next_sections = [
            html_text.find(s, start_idx + 1)
            for s in sections if html_text.find(s, start_idx + 1) != -1
        ]
        end_idx = min(next_sections) if next_sections else len(html_text)
        return html_text[start_idx:end_idx]

    # í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
    def make_prompt(subject, chapter, section, content):
        return f"""
ë‹¹ì‹ ì€ ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 1ê¸‰ í•„ê¸° êµì¬ë¥¼ ì§‘í•„í•˜ëŠ” ì „ë¬¸ ì €ìì…ë‹ˆë‹¤.

ì§€ê¸ˆë¶€í„° ì œê³µí•˜ëŠ” '{subject} {chapter} {section}' êµì¬ ì›ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜í—˜ìƒì´ í•™ìŠµí•˜ê¸° ì¢‹ë„ë¡ **êµê³¼ì„œ ìŠ¤íƒ€ì¼ì˜ ìš”ì•½ ì›ê³ **ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:

1. ì „ì²´ ë‚´ìš©ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì—¬ ì‘ì„±í•©ë‹ˆë‹¤.  
2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, **ê°œë… ì •ë¦¬, ë‹¨ê³„ë³„ ì„¤ëª…, ì˜ˆì‹œ, ë„ì‹ í˜•íƒœ ì„¤ëª… ë“±**ì„ í¬í•¨í•©ë‹ˆë‹¤.  
3. ê°€ê¸‰ì ì´ë©´ ë¬¸ë‹¨ì„ ë‚˜ëˆ  ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤.  
4. ì œëª©, ì†Œì œëª©, ê¸€ë¨¸ë¦¬í‘œ ë“±ì„ í™œìš©í•˜ì—¬ êµì¬ì²˜ëŸ¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.  
5. ë¬¸ì²´ëŠ” ìˆ˜í—˜ êµì¬ì— ë§ê²Œ **ì •ì¤‘í•˜ê³  ì„¤ëª… ìœ„ì£¼**ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

### êµì¬ ì›ë¬¸:
{content}

### ìš”ì•½ ì›ê³  (êµì¬ í˜•ì‹):
"""

    # GPT í˜¸ì¶œ í•¨ìˆ˜
    def gpt_summarize(prompt):
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1500
        )
        return response.choices[0].message["content"]

    # ì ˆ ì„ íƒ
    selected_sections = st.multiselect("ìš”ì•½í•  ì ˆì„ ì„ íƒí•˜ì„¸ìš”", options=sections, default=sections[:3])

    if st.button("ğŸ“˜ ìš”ì•½ ìƒì„±"):
        all_outputs = {}
        for sec in selected_sections:
            extracted = extract_section(sec)
            if extracted:
                prompt = make_prompt(subject, chapter, sec, extracted)
                with st.spinner(f"{sec} ìš”ì•½ ì¤‘..."):
                    try:
                        result = gpt_summarize(prompt)
                        st.subheader(f"ğŸ“˜ {sec.replace('</h1>', '')}")
                        st.write(result)
                        all_outputs[sec.replace("</h1>", "").strip()] = result
                    except Exception as e:
                        st.error(f"[âŒ ì˜¤ë¥˜] {sec} ìš”ì•½ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            else:
                st.warning(f"[!] '{sec}' ì ˆ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        if all_outputs:
            output_json = json.dumps(all_outputs, ensure_ascii=False, indent=2)
            st.download_button("ğŸ“¥ ìš”ì•½ ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ", output_json, file_name="summary_output.json", mime="application/json")


import os
import json
import time
import requests
import streamlit as st
from PyPDF2 import PdfReader, PdfWriter
import fitz  # PyMuPDF

# ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_DIR = "./"
UPLOAD_DIR = os.path.join(BASE_DIR, "uploads")
SPLIT_DIR = os.path.join(BASE_DIR, "splits")
JSON_DIR = os.path.join(BASE_DIR, "jsons")
RESULT_DIR = os.path.join(BASE_DIR, "results")

for d in [UPLOAD_DIR, SPLIT_DIR, JSON_DIR, RESULT_DIR]:
    os.makedirs(d, exist_ok=True)

# secrets.tomlì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
API_KEY = st.secrets["api"]["upstage_key"]
OCR_URL = "https://api.upstage.ai/v1/document-digitization"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

# ê³ ê¸‰ ë¶„í•  ê°œìˆ˜ ì¶”ì²œ í•¨ìˆ˜
def recommend_split_count_advanced(pdf_path):
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)
    reader = PdfReader(pdf_path)
    total_pages = len(reader.pages)
    avg_size_per_page = file_size_mb / total_pages if total_pages else 0

    doc = fitz.open(pdf_path)
    image_page_count = sum(1 for page in doc if page.get_images())
    image_ratio = image_page_count / total_pages if total_pages else 0
    doc.close()

    recommended = 8
    if total_pages <= 10:
        recommended = 1
    elif total_pages <= 30:
        recommended = 3
    elif total_pages <= 60:
        recommended = 6
    elif total_pages <= 100:
        recommended = 8
    elif total_pages <= 150:
        recommended = 10
    else:
        recommended = min(15, total_pages // 10)

    if avg_size_per_page > 1.5:
        recommended += 2
    elif avg_size_per_page > 1.0:
        recommended += 1

    if image_ratio > 0.7:
        recommended += 2
    elif image_ratio > 0.4:
        recommended += 1

    return min(recommended, total_pages)

# PDF ë¶„í•  ë²”ìœ„ ê³„ì‚°
def generate_split_ranges(total_pages, num_parts):
    base = total_pages // num_parts
    ranges = []
    for i in range(num_parts):
        start = i * base + 1
        end = (i + 1) * base if i < num_parts - 1 else total_pages
        ranges.append((start, end))
    return ranges

# PDF ë¶„í• 
def split_pdf(input_path, output_dir, num_parts):
    reader = PdfReader(input_path)
    total_pages = len(reader.pages)
    split_ranges = generate_split_ranges(total_pages, num_parts)
    split_paths = []

    for idx, (start, end) in enumerate(split_ranges):
        writer = PdfWriter()
        for page_num in range(start - 1, end):
            writer.add_page(reader.pages[page_num])
        output_pdf_path = os.path.join(output_dir, f"split_{idx+1}.pdf")
        with open(output_pdf_path, "wb") as f:
            writer.write(f)
        split_paths.append(output_pdf_path)
    return split_paths

# OCR API í˜¸ì¶œ ë° ì €ì¥ (ì¬ì‹œë„ í¬í•¨)
def call_api_until_success(pdf_path, output_json_path, max_retries=5):
    for attempt in range(max_retries):
        try:
            with open(pdf_path, "rb") as f:
                files = {"document": f}
                data = {"ocr": "force", "base64_encoding": "['table']", "model": "document-parse"}
                response = requests.post(OCR_URL, headers=HEADERS, files=files, data=data)

            if response.status_code == 200:
                result = response.json()
                with open(output_json_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                return True
        except Exception as e:
            st.warning(f"ì˜ˆì™¸ ë°œìƒ: {e}")
        time.sleep(2)
    return False

# JSON ë³‘í•©
def merge_jsons(input_dir, output_path):
    merged_html = ""
    for filename in sorted(os.listdir(input_dir)):
        if filename.endswith(".json"):
            with open(os.path.join(input_dir, filename), "r", encoding="utf-8") as f:
                data = json.load(f)
                try:
                    html = data["content"]["html"]
                    merged_html += html + "\n"
                except KeyError:
                    st.warning(f"HTML ëˆ„ë½: {filename}")
    result = {"api": "2.0", "content": {"html": merged_html}}
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    return output_path

# Streamlit UI
st.set_page_config(page_title="ì»´í™œ ìš”ì•½ì§‘ OCR ìë™í™”", layout="wide")
st.title("ğŸ“„ OCR ìë™í™”ê¸°ê¸°")

uploaded_file = st.file_uploader("1. PDF íŒŒì¼ ì—…ë¡œë“œ", type="pdf")

if uploaded_file:
    pdf_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.read())
    st.success("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ")

    # ì¶”ì²œ ë¶„í•  ê°œìˆ˜
    recommended = recommend_split_count_advanced(pdf_path)
    st.info(f"ğŸ” ì¶”ì²œ ì•ˆì „ ë¶„í•  ê°œìˆ˜: {recommended}ê°œ (í˜ì´ì§€ ìˆ˜, ì´ë¯¸ì§€ ë¹„ìœ¨, í•´ìƒë„ ê¸°ì¤€)")

    num_parts = st.slider("2. ë¶„í•  ê°œìˆ˜ ì„ íƒ", min_value=1, max_value=20, value=recommended)

    if st.button("3. OCR ì‹œì‘"):
        st.info("ğŸ”§ PDF ë¶„í•  ì¤‘...")
        split_paths = split_pdf(pdf_path, SPLIT_DIR, num_parts)
        st.success(f"ğŸ“„ ì´ {len(split_paths)}ê°œë¡œ ë¶„í•  ì™„ë£Œ")

        for i, path in enumerate(split_paths):
            json_path = os.path.join(JSON_DIR, f"split_{i+1}.json")
            with st.spinner(f"ğŸ” OCR ì¤‘: split_{i+1}.pdf"):
                success = call_api_until_success(path, json_path)
                if success:
                    st.success(f"âœ… ì™„ë£Œ: split_{i+1}")
                else:
                    st.error(f"âŒ ì‹¤íŒ¨: split_{i+1}")

        st.info("ğŸ§© OCR ê²°ê³¼ ë³‘í•© ì¤‘...")
        merged_path = os.path.join(RESULT_DIR, "merged_output.json")
        merged = merge_jsons(JSON_DIR, merged_path)
        st.success("âœ… ë³‘í•© ì™„ë£Œ")

        with open(merged, "rb") as f:
            st.download_button("ğŸ“¥ ë³‘í•©ëœ JSON ë‹¤ìš´ë¡œë“œ", f, file_name="merged_output.json", mime="application/json")





from openai import OpenAI

client = OpenAI(api_key=st.secrets["openai"]["api_key"])

# ì œëª©
st.title("ğŸ§  ì»´í™œ ìš”ì•½ì§‘ ì›ê³  ìë™ ìƒì„±ê¸° (GPT)")

# JSON ì—…ë¡œë“œ
json_file = st.file_uploader("ğŸ“¤ OCR ê²°ê³¼ merged_output.json ì—…ë¡œë“œ", type="json")
if json_file:
    data = json.load(json_file)
    html_text = data["content"]["html"]

    # ê³¼ëª©/ì¥ ì •ë³´ ì…ë ¥
    subject = st.text_input("ê³¼ëª©", "1ê³¼ëª©")
    chapter = st.text_input("ì¥ ì •ë³´", "2ì¥, 3ì¥, 4ì¥, 5ì¥")

    sections = [
        "Windowsì˜ ê¸°ì´ˆ", "ë°”íƒ• í™”ë©´", "íŒŒì¼ íƒìƒ‰ê¸°", "Windows ë³´ì¡°í”„ë¡œê·¸ë¨", "ì¸ì‡„", "ì„¤ì •</h1>", "ìœ Â·ë¬´ì„  ë„¤íŠ¸ì›Œí¬ ì„¤ì •</h1>", "ì»´í“¨í„°ì˜ ê°œë… ë° ì›ë¦¬</h1>", "ì»´í“¨í„°ì˜ ë°œì „ ê³¼ì •</h1>",
        "ì»´í“¨í„°ì˜ ë¶„ë¥˜</h1>", "ìë£Œì˜ í‘œí˜„ ë° ì²˜ë¦¬ ë°©ì‹</h1>", "ìˆ˜ì˜ í‘œí˜„ ë° ì—°ì‚°</h1>", "ì¤‘ì•™ ì²˜ë¦¬ ì¥ì¹˜</h1>",
        "ê¸°ì–µ ì¥ì¹˜ì˜ êµ¬ì„±</h1>", "ì…ì¶œë ¥ ì¥ì¹˜</h1>", "ê¸°íƒ€ ì¥ì¹˜</h1>", "ì†Œí”„íŠ¸ì›¨ì–´</h1>", "ìœ í‹¸ë¦¬í‹°(Utility)</h1>",
        "í”„ë¡œê·¸ë˜ë° ì–¸ì–´</h1>", "PC ìœ ì§€ì™€ ë³´ìˆ˜</h1>", "Windowsì—ì„œ PC ê´€ë¦¬\n", "ì¸í„°ë„· ì¼ë°˜</h1>",
        "ì¸í„°ë„· ì„œë¹„ìŠ¤</h1>", "ë©€í‹°ë¯¸ë””ì–´ì˜ ê°œë…\n", "ë©€í‹°ë¯¸ë””ì–´ì˜ ìš´ìš©</h1>", "ì •ë³´ í†µì‹  ì¼ë°˜</h1>",
        "ì •ë³´ ìœ¤ë¦¬</h1>", "ì»´í“¨í„° ë²”ì£„</h1>", "ë°”ì´ëŸ¬ìŠ¤ ì˜ˆë°©ê³¼ ì¹˜ë£Œ</h1>","ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ê°œìš”</h1>",
    "íŒŒì¼ ê´€ë¦¬</h1>",
    "ì›Œí¬ì‹œíŠ¸ì˜ ê´€ë¦¬</h1>",
    "ë°ì´í„° ì…ë ¥</h1>",
    "ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜</h1>",
    "[Excel ì˜µì…˜] ëŒ€í™” ìƒì</h1>",
    "ë°ì´í„° í¸ì§‘</h1>",
    "ì…€ í¸ì§‘</h1>",
    "ì…€ ì„œì‹ ë° ì‚¬ìš©ì ì§€ì • í‘œì‹œ í˜•ì‹</h1>",
    "ì„œì‹ ì„¤ì •\n",
    "ìˆ˜ì‹ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•\n",
    "ì…€ ì°¸ì¡°</h1>",
    "í•¨ìˆ˜ì˜ ê¸°ë³¸ ê°œë…\n",
    "ìˆ˜í•™ê³¼ ì‚¼ê° í•¨ìˆ˜/ë‚ ì§œì™€ ì‹œê°„ í•¨ìˆ˜\n",
    "ë¬¸ìì—´ í•¨ìˆ˜</h1>",
    "ì°¾ê¸°ì™€ ì°¸ì¡° í•¨ìˆ˜</h1>",
    "D í•¨ìˆ˜/ì¬ë¬´ í•¨ìˆ˜/ì •ë³´ í•¨ìˆ˜</h1>",
    "ë°°ì—´ê³¼ ë°°ì—´ ìˆ˜ì‹</h1>",
    "ë°°ì—´ í•¨ìˆ˜\n",
    "ì •ë ¬</h1>",
    "í•„í„° ê¸°ëŠ¥</h1>",
    "ê¸°íƒ€ ë°ì´í„° ê´€ë¦¬ ê¸°ëŠ¥</h1>",
    "ë°ì´í„° ê°€ì ¸ì˜¤ê¸°</h1>",
    "ë¶€ë¶„í•©/ë°ì´í„° í‘œ/ë°ì´í„° í†µí•©</h1>",
    "í”¼ë²— í…Œì´ë¸”</h1>",
    "í”¼ë²— ì°¨íŠ¸</h1>",
    "ëª©í‘œê°’ ì°¾ê¸°/ì‹œë‚˜ë¦¬ì˜¤</h1>",
    "ì¸ì‡„\ní•©ê²©",
    "í˜ì´ì§€ ì„¤ì •</h1>",
    "ë¦¬ë³¸ ë©”ë‰´ì™€ ì°½ ë‹¤ë£¨ê¸°</h1>",
    "ì°¨íŠ¸ì˜ ê¸°ë³¸</h1>",
    "ì°¨íŠ¸ì˜ ì¢…ë¥˜</h1>",
    "ì°¨íŠ¸ í¸ì§‘</h1>",
    "ì°¨íŠ¸ì˜ ìš”ì†Œ ì¶”ê°€ì™€ ì„œì‹ ì§€ì •</h1>",
    "ë§¤í¬ë¡œ ì‘ì„±</h1>",
    "VBA í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ë³¸ ê°œë…</h1>",
    "VBA ë¬¸ë²•\n",
    "ê°œì²´ ì†ì„± ë° ì»¨íŠ¸ë¡¤ ì†ì„±</h1>","ë°ì´í„°ë² ì´ìŠ¤ì˜ ê°œë…ê³¼ ìš©ì–´</h1>",
    "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„</h1>",
    "ì•¡ì„¸ìŠ¤ ì‚¬ìš©ì˜ ê¸°ì´ˆ</h1>",
    "í…Œì´ë¸” ìƒì„±</h1>",
    "í…Œì´ë¸” ìˆ˜ì •</h1>",
    "í•„ë“œ ì†ì„± 1-ì†ì„±ê³¼ í˜•ì‹</h1>",
    "í•„ë“œ ì†ì„± 2-ì…ë ¥ ë§ˆìŠ¤í¬/ì¡°íšŒ ì†ì„±</h1>",
    "í•„ë“œ ì†ì„± 3-ìœ íš¨ì„± ê²€ì‚¬/ê¸°íƒ€ í•„ë“œ<br>ì†ì„±/ê¸°ë³¸í‚¤/ì¸ë±ìŠ¤</h1>",
    "í•„ë“œ ì†ì„± 4-ê´€ê³„ ì„¤ì •/ì°¸ì¡° ë¬´ê²°ì„±</h1>",
    "ë°ì´í„° ì…ë ¥</h1>",
    "ë°ì´í„° ë‚´ë³´ë‚´ê¸°</h1>",
    "ì¿¼ë¦¬(Query)</h1>",
    "ë‹¨ìˆœ ì¡°íšŒ ì¿¼ë¦¬(SQLë¬¸)</h1>",
    "ì‹ì˜ ì‚¬ìš©\n",
    "ë‹¤ì¤‘ í…Œì´ë¸”ì„ ì´ìš©í•œ ì¿¼ë¦¬</h1>",
    "ì‹¤í–‰ ì¿¼ë¦¬(Action Query)</h1",
    "ê¸°íƒ€ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬</h1>",
    "í¼ ì‘ì„± ê¸°ë³¸</h1>",
    "í¼ì˜ ì£¼ìš” ì†ì„±</h1>",
    "í•˜ìœ„ í¼</h1>",
    "ì»¨íŠ¸ë¡¤ì˜ ì‚¬ìš© 1-ì»¨íŠ¸ë¡¤ì˜ ê°œë…/<br>ì»¨íŠ¸ë¡¤ ë§Œë“¤ê¸°</h1>",
    "ì»¨íŠ¸ë¡¤ì˜ ì‚¬ìš© 2-ì»¨íŠ¸ë¡¤ ë‹¤ë£¨ê¸°/<br>ì»¨íŠ¸ë¡¤ì˜ ì£¼ìš” ì†ì„±</h1>",
    "í¼ ì‘ì„± ê¸°íƒ€</h1>",
    "ë³´ê³ ì„œ ì‘ì„±ê³¼ ì¸ì‡„</h1>",
    "ë³´ê³ ì„œ êµ¬ì—­ ë° ê·¸ë£¹í™”</h1>",
    "ë‹¤ì–‘í•œ ë³´ê³ ì„œ ì‘ì„±</h1>",
    "ë³´ê³ ì„œ ì‘ì„± ê¸°íƒ€</h1>",
    "ë§¤í¬ë¡œì˜ í™œìš© 1<br>ë§¤í¬ë¡œ í•¨ìˆ˜ì˜ ê°œë…/ë§¤í¬ë¡œ ë§Œë“¤ê¸°</h1>",
    "ë§¤í¬ë¡œì˜ í™œìš© 2-<br>ì‹¤í–‰/ìˆ˜ì •/ì£¼ìš” ë§¤í¬ë¡œ í•¨ìˆ˜</h1>",
    "VBAë¥¼ ì´ìš©í•œ ëª¨ë“ˆ ì‘ì„±</h1>"
    ]

    # ì ˆ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
    def extract_section(section_title):
        start_idx = html_text.find(section_title)
        if start_idx == -1:
            return None
        next_sections = [
            html_text.find(s, start_idx + 1)
            for s in sections if html_text.find(s, start_idx + 1) != -1
        ]
        end_idx = min(next_sections) if next_sections else len(html_text)
        return html_text[start_idx:end_idx]

    # í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
    def make_prompt(subject, chapter, section, content):
        return f"""
ë‹¹ì‹ ì€ ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 1ê¸‰ í•„ê¸° êµì¬ë¥¼ ì§‘í•„í•˜ëŠ” ì „ë¬¸ ì €ìì…ë‹ˆë‹¤.

ì§€ê¸ˆë¶€í„° ì œê³µí•˜ëŠ” '{subject} {chapter} {section}' êµì¬ ì›ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜í—˜ìƒì´ í•™ìŠµí•˜ê¸° ì¢‹ë„ë¡ **êµê³¼ì„œ ìŠ¤íƒ€ì¼ì˜ ìš”ì•½ ì›ê³ **ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:

1. ì „ì²´ ë‚´ìš©ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì—¬ ì‘ì„±í•©ë‹ˆë‹¤.  
2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, **ê°œë… ì •ë¦¬, ë‹¨ê³„ë³„ ì„¤ëª…, ì˜ˆì‹œ, ë„ì‹ í˜•íƒœ ì„¤ëª… ë“±**ì„ í¬í•¨í•©ë‹ˆë‹¤.  
3. ê°€ê¸‰ì ì´ë©´ ë¬¸ë‹¨ì„ ë‚˜ëˆ  ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤.  
4. ì œëª©, ì†Œì œëª©, ê¸€ë¨¸ë¦¬í‘œ ë“±ì„ í™œìš©í•˜ì—¬ êµì¬ì²˜ëŸ¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.  
5. ë¬¸ì²´ëŠ” ìˆ˜í—˜ êµì¬ì— ë§ê²Œ **ì •ì¤‘í•˜ê³  ì„¤ëª… ìœ„ì£¼**ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

### êµì¬ ì›ë¬¸:
{content}

### ìš”ì•½ ì›ê³  (êµì¬ í˜•ì‹):
"""

    # GPT í˜¸ì¶œ í•¨ìˆ˜
    def gpt_summarize(prompt):
        from openai import OpenAI
        client = OpenAI(api_key=st.secrets["openai"]["api_key"])
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1500
        )
        return response.choices[0].message.content

    # ì ˆ ì„ íƒ
    selected_sections = st.multiselect("ìš”ì•½í•  ì ˆì„ ì„ íƒí•˜ì„¸ìš”", options=sections, default=sections[:3])

    if st.button("ğŸ“˜ ìš”ì•½ ìƒì„±"):
        all_outputs = {}
        for sec in selected_sections:
            extracted = extract_section(sec)
            if extracted:
                prompt = make_prompt(subject, chapter, sec, extracted)
                with st.spinner(f"{sec} ìš”ì•½ ì¤‘..."):
                    try:
                        result = gpt_summarize(prompt)
                        st.subheader(f"ğŸ“˜ {sec.replace('</h1>', '')}")
                        st.write(result)
                        all_outputs[sec.replace("</h1>", "").strip()] = result
                    except Exception as e:
                        st.error(f"[âŒ ì˜¤ë¥˜] {sec} ìš”ì•½ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            else:
                st.warning(f"[!] '{sec}' ì ˆ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        if all_outputs:
            output_json = json.dumps(all_outputs, ensure_ascii=False, indent=2)
            st.download_button("ğŸ“¥ ìš”ì•½ ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ", output_json, file_name="summary_output.json", mime="application/json")



import streamlit as st
import json
import re
from openai import OpenAI

# ğŸ” API Key from secrets.toml
client = OpenAI(api_key=st.secrets["openai"]["api_key"])
GPT_MODEL = "gpt-4o"

st.title("ğŸ“˜ ìš”ì•½ ì›ê³  ì €ì¥ + êµì¬ ìŠ¤íƒ€ì¼ ê°€ê³µ")

# âœ… 1ë‹¨ê³„: JSON ì—…ë¡œë“œ ë˜ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìë™ ì „ë‹¬
st.header("â‘  ìš”ì•½ ì›ê³  ë¶ˆëŸ¬ì˜¤ê¸°")

uploaded_json = st.file_uploader("ğŸ“¤ ìš”ì•½ ê²°ê³¼ JSON ì—…ë¡œë“œ (ë˜ëŠ” ìë™ ìƒì„±)", type="json")

if "summary_json" not in st.session_state:
    st.session_state.summary_json = None

if uploaded_json:
    st.session_state.summary_json = json.load(uploaded_json)
elif st.session_state.summary_json:
    st.success("âœ… ì´ì „ ë‹¨ê³„ì—ì„œ ìë™ìœ¼ë¡œ ìš”ì•½ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
else:
    st.warning("ìš”ì•½ JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.")

# âœ… 2ë‹¨ê³„: ì €ì¥ ê°€ëŠ¥í•œ TXT ë³€í™˜
if st.session_state.summary_json:
    json_data = st.session_state.summary_json
    sections_txt = ""

    for i, (title, content) in enumerate(json_data.items()):
        sections_txt += f"\n\n===== {title} ìš”ì•½ ê²°ê³¼ =====\n\n{content}\n"

    # ì €ì¥ ë²„íŠ¼
    st.download_button(
        "ğŸ“¥ ìš”ì•½ ê²°ê³¼ TXT ë‹¤ìš´ë¡œë“œ",
        sections_txt,
        file_name="summary_output.txt",
        mime="text/plain"
    )

    st.download_button(
        "ğŸ“¥ ì›ë³¸ JSON ë‹¤ìš´ë¡œë“œ",
        json.dumps(json_data, ensure_ascii=False, indent=2),
        file_name="summary_output.json",
        mime="application/json"
    )

    st.markdown("---")

    # âœ… 3ë‹¨ê³„: ìë™ ì—°ê²° (2ì°¨ ê°€ê³µ)
    st.header("â‘¡ GPT ê¸°ë°˜ êµì¬ ìŠ¤íƒ€ì¼ ë‹¤ë“¬ê¸°")

    # ì„¹ì…˜ ë‚˜ëˆ„ê¸°
    def extract_sections(text):
        split_sections = re.split(r'^={5}.*?={5}\s*$', text, flags=re.MULTILINE)
        titles = re.findall(r'^={5}\s*(.*?)\s*(?:</h1>)?\s*ìš”ì•½ ê²°ê³¼\s*={5}', text, flags=re.MULTILINE)
        return list(zip(titles, [s.strip() for s in split_sections if s.strip()]))

    sections = extract_sections(sections_txt)
    st.success(f"âœ… ì´ {len(sections)}ê°œ ì ˆì„ ê°€ê³µí•©ë‹ˆë‹¤.")

    results = {}
    for i, (title, content) in enumerate(sections):
        with st.expander(f"ğŸ“˜ [{i+1}] {title} - GPT ì¬ì‘ì„±"):
            with st.spinner(f"ğŸ” GPTë¡œ '{title}' ë‹¤ë“¬ëŠ” ì¤‘..."):
                try:
                    # gpt ì¬ì‘ì„± í•¨ìˆ˜
                    def ask_gpt(title, content):
                        prompt = f"[ë¬¸ë‹¨ ì œëª©]\n{title}\n\n[ë‚´ìš©]\n{content}"
                        system_prompt = """
ë‹¹ì‹ ì€ êµì¬ë¥¼ ì§‘í•„í•˜ëŠ” ì €ìì…ë‹ˆë‹¤. ì•„ë˜ ì›ê³  ë¬¸ë‹¨ì€ êµì¬ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.  
ê° ë¬¸ë‹¨ì€ ì™„ê²°ëœ ë‹¨ì›ìœ¼ë¡œ ì±…ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ **ì™„ì„±ëœ ì¶œíŒìš© ë ˆì´ì•„ì›ƒê³¼ ì„œìˆ  ë°©ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ì¤‘ìš” ì§€ì¹¨]
1. ì‹¤ì œ êµì¬ êµ¬ì„± í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
2. í•µì‹¬ ê°œë…, ìƒì„¸ ì„¤ëª…, í‘œ/ë„ì‹, í•™ìŠµ ë„ìš°ë¯¸ ë“± êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
3. ì´ë¯¸ì§€ê°€ í•„ìš”í•œ ê²½ìš°, ìœ„ì¹˜/ì„¤ëª…/í¬ê¸°/ìº¡ì…˜ í¬í•¨

[ì¶œë ¥ í¬ë§·]
ğŸ“˜ ë‹¨ì› ì œëª©: {ë¬¸ë‹¨ ì œëª©} / ë‚œì´ë„:â­â­â­â­â­
## âœ¨ í•µì‹¬ ê°œë…
- ...
## ğŸ“– ìƒì„¸ ì„¤ëª…
...
"""
                        response = client.chat.completions.create(
                            model=GPT_MODEL,
                            messages=[
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            max_tokens=2000,
                            temperature=0.3
                        )
                        return response.choices[0].message.content.strip()

                    refined = ask_gpt(title, content)
                    st.markdown(refined, unsafe_allow_html=True)
                    results[title] = refined

                except Exception as e:
                    st.error(f"âŒ ì˜¤ë¥˜: {e}")

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if results:
        full_refined_txt = ""
        for title, body in results.items():
            full_refined_txt += f"\n\n===== {title} ìš”ì•½ ê²°ê³¼ =====\n\n{body}\n"

        st.download_button(
            "ğŸ“¥ ìµœì¢… êµì¬ ìŠ¤íƒ€ì¼ TXT ë‹¤ìš´ë¡œë“œ",
            full_refined_txt,
            file_name="refined_textbook.txt",
            mime="text/plain"
        )

        st.download_button(
            "ğŸ“¥ ìµœì¢… JSON ë‹¤ìš´ë¡œë“œ",
            json.dumps(results, ensure_ascii=False, indent=2),
            file_name="refined_textbook.json",
            mime="application/json"
        )


